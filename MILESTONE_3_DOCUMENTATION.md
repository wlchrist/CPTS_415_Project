# Project Milestone 3 Documentation

## 1. Data Files

### 1a. Distributed Data Files

**Primary Storage: MongoDB (NoSQL Database)**

The project uses MongoDB as the primary NoSQL database for data storage. MongoDB is a document-oriented database that provides distributed storage capabilities through sharding and replication.

**Additional Data Files:**

1. **CSV Files (Intermediate Storage)**
   - `ratings.csv`: Contains user ratings data with columns: `userID`, `movieID`, `title`, `genre`, `rating`
   - Used as an intermediate format for data loading into MongoDB
   - Not a distributed file system, but serves as the source data file

2. **JSON Files (Document Format)**
   - `movie_documents.json`: Contains transformed movie rating documents in JSON format
   - Represents the document structure used in MongoDB
   - Generated by the transformation pipeline

3. **Parquet Files (Distributed File Format)**
   - Generated by `spark_processing.py` for efficient distributed storage
   - Stored in `spark_output/ratings_processed.parquet`
   - Columnar format optimized for Spark processing
   - Supports distributed storage and processing

**Note:** The project uses both MongoDB (NoSQL) and Spark with Parquet files for distributed processing, meeting the requirement for both NoSQL and Hadoop/Spark frameworks.

### 1b. Data Transformation Steps

The data transformation pipeline follows these steps:

1. **Data Extraction** (`process_imdb_data.py`)
   - Reads IMDb Title Basics TSV file
   - Filters for movie entries only
   - Processes data in chunks (10,000 records per chunk) for memory efficiency
   - Generates synthetic ratings data
   - Outputs: `ratings.csv`

2. **Data Reduction** (`reduction.py`)
   - Optionally reduces dataset size by limiting number of users and/or movies
   - Filters dataset to include only specified user IDs and movie IDs
   - Input: Full dataset from CSV
   - Output: Reduced dataset

3. **Data Cleansing** (`cleansing.py`)
   - Removes rows with null values in critical fields (userID, movieID, title, genre, rating)
   - Validates rating values (must be between 0 and 5)
   - Strips whitespace from genre field
   - Converts rating to integer type
   - Input: Raw or reduced dataset
   - Output: Cleaned dataset

4. **Data Transformation** (`transformation.py`)
   - Groups data by userID
   - Transforms flat CSV structure into nested document structure
   - Creates user-centric documents with embedded rating arrays
   - Input: Cleaned dataset
   - Output: JSON document array

5. **MongoDB Document Creation** (`mongodb_database.py`)
   - Creates three document types: users, movies, and ratings
   - Aggregates statistics (average ratings, rating distributions)
   - Creates indexes for query optimization
   - Input: Cleaned CSV data
   - Output: MongoDB collections with indexed documents

6. **Spark Data Processing** (`spark_processing.py`)
   - Loads data into Spark DataFrames
   - Applies distributed reduction and cleansing using Spark operations
   - Performs map/filter/reduce operations for data transformation
   - Generates aggregations using Spark's distributed computing
   - Outputs: Parquet files for distributed storage
   - Input: CSV files or MongoDB data
   - Output: Processed Parquet files and aggregated statistics

7. **Spark MLlib Collaborative Filtering** (`spark_mllib_recommendations.py`)
   - Implements Alternating Least Squares (ALS) algorithm
   - Trains recommendation model using Spark MLlib
   - Generates movie recommendations for users
   - Evaluates model performance using RMSE
   - Input: Ratings data
   - Output: Trained model and recommendations

8. **MongoDB-Spark Integration** (`mongodb_spark_integration.py`)
   - Reads data from MongoDB into Spark
   - Processes data using Spark operations
   - Writes Spark results back to MongoDB
   - Demonstrates bidirectional data flow between NoSQL and Spark

### 1c. Sample Data

**Sample CSV Data (ratings.csv):**
```csv
userID,movieID,title,genre,rating
61465,1109,Godina igre,Documentary,2
582073,3687,War of the Crowns: Fashion vs Beauty,Documentary,3
660847,4068,Young and Famous,Documentary,5
773016,4635,Centoquaranta - La strage dimenticata,Documentary,3
```

**Sample JSON Document (movie_documents.json):**
```json
{
  "userID": 1591,
  "ratings": [
    {
      "movieID": 128,
      "title": "Croaker",
      "genre": "Horror",
      "rating": 2.0
    }
  ]
}
```

**Sample MongoDB User Document:**
```json
{
  "user_id": 1591,
  "total_ratings": 1,
  "average_rating": 2.0,
  "rating_history": [
    {
      "movie_id": 128,
      "title": "Croaker",
      "genre": "Horror",
      "rating": 2
    }
  ]
}
```

**Sample MongoDB Movie Document:**
```json
{
  "movie_id": 128,
  "title": "Croaker",
  "genre": "Horror",
  "total_ratings": 1,
  "average_rating": 2.0,
  "rating_distribution": {
    "1": 0,
    "2": 1,
    "3": 0,
    "4": 0,
    "5": 0
  }
}
```

---

## 2. Algorithm Description

### 2a. Formal Algorithm Descriptions

#### Algorithm 1: Data Reduction

**Input:**
- `dataset`: DataFrame containing movie ratings with columns (userID, movieID, title, genre, rating)
- `max_users`: Optional integer limiting the number of unique users
- `max_movies`: Optional integer limiting the number of unique movies

**Output:**
- `reduced_dataset`: DataFrame containing filtered ratings based on user and movie limits

**Computing Operations:**
1. Extract unique user IDs from dataset
2. If `max_users` is specified, select first `max_users` unique user IDs
3. Filter dataset to include only selected user IDs
4. Extract unique movie IDs from filtered dataset
5. If `max_movies` is specified, select first `max_movies` unique movie IDs
6. Filter dataset to include only selected movie IDs
7. Return filtered dataset

**Pseudo-code:**
```
FUNCTION reduce(dataset, max_users, max_movies):
    IF max_users IS NOT NULL:
        unique_users = GET_UNIQUE_VALUES(dataset['userID'])
        selected_users = unique_users[0:max_users]
        dataset = FILTER(dataset, userID IN selected_users)
    
    IF max_movies IS NOT NULL:
        unique_movies = GET_UNIQUE_VALUES(dataset['movieID'])
        selected_movies = unique_movies[0:max_movies]
        dataset = FILTER(dataset, movieID IN selected_movies)
    
    RETURN dataset
END FUNCTION
```

#### Algorithm 2: Data Cleansing

**Input:**
- `dataset`: DataFrame containing movie ratings with columns (userID, movieID, title, genre, rating)

**Output:**
- `cleansed_dataset`: DataFrame with validated and normalized data

**Computing Operations:**
1. Remove rows where any of (userID, movieID, title, genre, rating) is null
2. Filter rows where rating is between 0 and 5 (inclusive)
3. Strip leading/trailing whitespace from genre field
4. Convert rating column to integer type
5. Return cleansed dataset

**Pseudo-code:**
```
FUNCTION cleanse(dataset):
    // Remove null values
    dataset = REMOVE_ROWS_WITH_NULL(dataset, columns=['userID', 'movieID', 'title', 'genre', 'rating'])
    
    // Validate rating range
    dataset = FILTER(dataset, rating >= 0 AND rating <= 5)
    
    // Normalize genre field
    dataset['genre'] = STRIP_WHITESPACE(dataset['genre'])
    
    // Type conversion
    dataset['rating'] = CONVERT_TO_INTEGER(dataset['rating'])
    
    RETURN dataset
END FUNCTION
```

#### Algorithm 3: Data Transformation

**Input:**
- `dataset`: Cleaned DataFrame with columns (userID, movieID, title, genre, rating)

**Output:**
- `documents`: List of JSON-like document objects, each representing a user with their ratings

**Computing Operations:**
1. Group dataset by userID
2. For each user group:
   a. Extract userID
   b. Create empty ratings array
   c. For each row in user group:
      - Create rating object with movieID, title, genre, rating
      - Append rating object to ratings array
   d. Create user document with userID and ratings array
   e. Append user document to documents list
3. Return documents list

**Pseudo-code:**
```
FUNCTION transform(dataset):
    documents = EMPTY_LIST()
    
    FOR EACH (user_id, user_group) IN GROUP_BY(dataset, 'userID'):
        ratings = EMPTY_LIST()
        
        FOR EACH row IN user_group:
            rating_object = {
                'movieID': row['movieID'],
                'title': row['title'],
                'genre': row['genre'],
                'rating': row['rating']
            }
            APPEND(ratings, rating_object)
        
        user_document = {
            'userID': user_id,
            'ratings': ratings
        }
        APPEND(documents, user_document)
    
    RETURN documents
END FUNCTION
```

#### Algorithm 4: MongoDB Document Creation (Users)

**Input:**
- `df`: Cleaned DataFrame with columns (userID, movieID, title, genre, rating)

**Output:**
- `user_documents`: List of user document objects ready for MongoDB insertion

**Computing Operations:**
1. Group DataFrame by userID
2. For each user group:
   a. Calculate total_ratings (count of rows)
   b. Calculate average_rating (mean of rating column)
   c. Create empty rating_history array
   d. For each row in user group:
      - Create rating_entry with movie_id, title, genre, rating
      - Append to rating_history
   e. Create user_doc with user_id, total_ratings, average_rating, rating_history
   f. Append user_doc to user_documents list
3. Return user_documents list

**Pseudo-code:**
```
FUNCTION create_user_documents(df):
    user_documents = EMPTY_LIST()
    
    FOR EACH (user_id, user_data) IN GROUP_BY(df, 'userID'):
        total_ratings = COUNT_ROWS(user_data)
        average_rating = MEAN(user_data['rating'])
        rating_history = EMPTY_LIST()
        
        FOR EACH row IN user_data:
            rating_entry = {
                'movie_id': row['movieID'],
                'title': row['title'],
                'genre': row['genre'],
                'rating': row['rating']
            }
            APPEND(rating_history, rating_entry)
        
        user_doc = {
            'user_id': user_id,
            'total_ratings': total_ratings,
            'average_rating': average_rating,
            'rating_history': rating_history
        }
        APPEND(user_documents, user_doc)
    
    RETURN user_documents
END FUNCTION
```

#### Algorithm 5: MongoDB Document Creation (Movies)

**Input:**
- `df`: Cleaned DataFrame with columns (userID, movieID, title, genre, rating)

**Output:**
- `movie_documents`: List of movie document objects ready for MongoDB insertion

**Computing Operations:**
1. Group DataFrame by movieID
2. For each movie group:
   a. Extract movie_id, title, genre (from first row)
   b. Calculate total_ratings (count of rows)
   c. Calculate average_rating (mean of rating column)
   d. Calculate rating_distribution (count of each rating value 1-5)
   e. Create movie_doc with all calculated fields
   f. Append movie_doc to movie_documents list
3. Return movie_documents list

**Pseudo-code:**
```
FUNCTION create_movie_documents(df):
    movie_documents = EMPTY_LIST()
    
    FOR EACH (movie_id, movie_data) IN GROUP_BY(df, 'movieID'):
        title = movie_data['title'][0]
        genre = movie_data['genre'][0]
        total_ratings = COUNT_ROWS(movie_data)
        average_rating = MEAN(movie_data['rating'])
        
        rating_distribution = {
            '1': COUNT(movie_data['rating'] == 1),
            '2': COUNT(movie_data['rating'] == 2),
            '3': COUNT(movie_data['rating'] == 3),
            '4': COUNT(movie_data['rating'] == 4),
            '5': COUNT(movie_data['rating'] == 5)
        }
        
        movie_doc = {
            'movie_id': movie_id,
            'title': title,
            'genre': genre,
            'total_ratings': total_ratings,
            'average_rating': average_rating,
            'rating_distribution': rating_distribution
        }
        APPEND(movie_documents, movie_doc)
    
    RETURN movie_documents
END FUNCTION
```

#### Algorithm 6: MongoDB Query with Aggregation

**Input:**
- `collection`: MongoDB collection object
- `pipeline`: Aggregation pipeline array with MongoDB aggregation stages

**Output:**
- `results`: List of aggregated document results

**Computing Operations:**
1. Execute aggregation pipeline on collection
2. Pipeline stages may include:
   - $group: Group documents by specified field
   - $sort: Sort grouped results
   - $limit: Limit number of results
   - $sum: Calculate sum of values
3. Convert cursor to list
4. Return results list

**Pseudo-code:**
```
FUNCTION aggregate(collection, pipeline):
    cursor = EXECUTE_AGGREGATION(collection, pipeline)
    results = CONVERT_TO_LIST(cursor)
    RETURN results
END FUNCTION

// Example: Top genres by count
pipeline = [
    {$group: {_id: "$genre", count: {$sum: 1}}},
    {$sort: {count: -1}},
    {$limit: 5}
]
results = aggregate(movies_collection, pipeline)
```

#### Algorithm 7: Spark Map/Filter/Reduce Operations

**Input:**
- `df`: Spark DataFrame containing movie ratings
- `max_users`: Optional integer limiting the number of users
- `max_movies`: Optional integer limiting the number of movies

**Output:**
- `processed_df`: Spark DataFrame with processed and aggregated data
- `parquet_files`: Distributed Parquet files for storage

**Computing Operations:**
1. Load data into Spark DataFrame (distributed across cluster)
2. **MAP**: Transform data by adding derived columns (rating categories)
3. **FILTER**: Apply distributed filtering operations (null removal, rating validation)
4. **REDUCE**: Aggregate data using groupBy operations (user stats, movie stats, genre stats)
5. Save results to Parquet format for distributed storage

**Pseudo-code:**
```
FUNCTION spark_process(df, max_users, max_movies):
    // MAP: Transform data
    df_mapped = df.withColumn("rating_category", 
        IF rating >= 4 THEN "High"
        ELSE IF rating >= 3 THEN "Medium"
        ELSE "Low")
    
    // FILTER: Remove invalid data
    df_filtered = df_mapped.filter(
        userID IS NOT NULL AND
        movieID IS NOT NULL AND
        rating BETWEEN 1 AND 5
    )
    
    // REDUCE: Aggregate statistics
    user_stats = df_filtered.groupBy("userID").agg(
        COUNT("rating") AS rating_count,
        AVG("rating") AS avg_rating
    )
    
    genre_stats = df_filtered.groupBy("genre", "rating_category").agg(
        COUNT("rating") AS count,
        AVG("rating") AS avg_rating
    )
    
    // Save to distributed storage
    df_filtered.write.parquet("spark_output/ratings.parquet")
    
    RETURN df_filtered, user_stats, genre_stats
END FUNCTION
```

#### Algorithm 8: Spark MLlib Collaborative Filtering (ALS)

**Input:**
- `ratings_df`: Spark DataFrame with columns (user_id, movie_id, rating)
- `rank`: Integer for matrix factorization rank (default: 10)
- `max_iter`: Maximum iterations for ALS (default: 10)
- `reg_param`: Regularization parameter (default: 0.1)

**Output:**
- `model`: Trained ALS recommendation model
- `predictions`: DataFrame with predicted ratings
- `recommendations`: Top-N movie recommendations for users
- `rmse`: Root Mean Squared Error metric

**Computing Operations:**
1. Split data into training (80%) and test (20%) sets
2. Initialize ALS model with specified parameters
3. Train model using Alternating Least Squares algorithm
4. Generate predictions on test set
5. Evaluate model using RMSE metric
6. Generate top-N recommendations for users
7. Return trained model and metrics

**Pseudo-code:**
```
FUNCTION train_als_model(ratings_df, rank, max_iter, reg_param):
    // Split data
    (training, test) = RANDOM_SPLIT(ratings_df, [0.8, 0.2])
    
    // Initialize ALS model
    als = ALS(
        rank = rank,
        maxIter = max_iter,
        regParam = reg_param,
        userCol = "user_id",
        itemCol = "movie_id",
        ratingCol = "rating"
    )
    
    // Train model
    model = als.fit(training)
    
    // Generate predictions
    predictions = model.transform(test)
    
    // Evaluate
    rmse = EVALUATE(predictions, metric="RMSE")
    
    // Generate recommendations
    recommendations = model.recommendForAllUsers(topN=10)
    
    RETURN model, predictions, recommendations, rmse
END FUNCTION
```

**Advanced Features:**
- Matrix factorization for collaborative filtering
- Handles cold-start problem with drop strategy
- Distributed training across Spark cluster
- Scalable to millions of users and movies

#### Algorithm 9: MongoDB-Spark Integration

**Input:**
- MongoDB connection string and collection name
- Spark session

**Output:**
- Spark DataFrame with MongoDB data
- MongoDB collections with Spark processing results

**Computing Operations:**
1. Connect to MongoDB
2. Read documents from MongoDB collection
3. Convert MongoDB documents to Spark DataFrame
4. Process data using Spark operations (aggregations, transformations)
5. Convert Spark results back to MongoDB format
6. Write results to MongoDB collections

**Pseudo-code:**
```
FUNCTION mongodb_spark_integration(mongodb_uri, collection_name):
    // Read from MongoDB
    client = CONNECT_MONGODB(mongodb_uri)
    documents = client[collection_name].find()
    df_pandas = CONVERT_TO_PANDAS(documents)
    df_spark = spark.createDataFrame(df_pandas)
    
    // Process in Spark
    user_stats = df_spark.groupBy("user_id").agg(
        COUNT("rating"), AVG("rating")
    )
    
    // Write back to MongoDB
    df_pandas = user_stats.toPandas()
    client["spark_results"].insert_many(df_pandas.to_dict('records'))
    
    RETURN df_spark, user_stats
END FUNCTION
```

### 2b. Optimization Techniques

1. **Database Indexing**
   - Created indexes on frequently queried fields:
     - Users collection: `user_id`, `average_rating`
     - Movies collection: `movie_id` (unique), `genre`, `average_rating`
     - Ratings collection: `user_id`, `movie_id`, `rating`
   - Indexes significantly improve query performance (measured in performance_measurement.py)

2. **Batch Insertions**
   - Uses `insert_many()` for bulk document insertion
   - Reduces network round-trips compared to individual inserts
   - Improves insertion performance by orders of magnitude

3. **Chunked Data Processing**
   - `process_imdb_data.py` processes large TSV files in chunks (10,000 records)
   - Prevents memory overflow when processing large datasets
   - Enables processing of datasets larger than available RAM

4. **Data Type Optimization**
   - Converts ratings to integers to reduce storage size
   - Uses appropriate MongoDB data types for efficient storage

5. **Query Optimization**
   - Uses MongoDB aggregation pipelines for complex queries
   - Leverages indexes for filter operations
   - Limits result sets when only top N results are needed

6. **Spark Distributed Processing**
   - Leverages Spark's distributed computing for large-scale processing
   - Uses Parquet columnar format for efficient storage and querying
   - Implements lazy evaluation for optimal execution plans
   - Partitions data across cluster nodes for parallel processing

7. **Spark MLlib Optimization**
   - Uses matrix factorization for efficient collaborative filtering
   - Implements ALS algorithm optimized for distributed systems
   - Handles sparse matrices efficiently
   - Supports model checkpointing for fault tolerance

---

## 3. Algorithm Results

### 3a. Algorithm Outputs

**Data Processing Pipeline Results:**

The complete pipeline processes data through reduction, cleansing, and transformation stages. Sample output from the transformation stage:

```json
[
  {
    "userID": 1591,
    "ratings": [
      {
        "movieID": 128,
        "title": "Croaker",
        "genre": "Horror",
        "rating": 2.0
      }
    ]
  },
  {
    "userID": 1640,
    "ratings": [
      {
        "movieID": 129,
        "title": "The Queen's Lost Uncle",
        "genre": "Biography",
        "rating": 4.0
      }
    ]
  }
]
```

**MongoDB Query Results:**

Example queries demonstrate various operations:

1. **Count Operations:**
   - Total users, movies, and ratings in database

2. **Filter Queries:**
   - Users with average rating >= 4.0
   - Movies in specific genres (e.g., "Drama")
   - Ratings with specific values (e.g., 5-star ratings)

3. **Aggregation Queries:**
   - Top 5 genres by movie count
   - Rating distribution across all ratings
   - Most popular movies by average rating

**Spark Processing Results:**

1. **Map/Filter/Reduce Operations:**
   - Distributed data processing across Spark cluster
   - Genre-rating category statistics
   - High-rated movie filtering (ratings >= 4)
   - Aggregated user and movie statistics

2. **Parquet File Output:**
   - Processed data saved to `spark_output/ratings_processed.parquet`
   - Columnar format for efficient querying
   - Distributed storage ready for large-scale processing

**Spark MLlib Results:**

1. **Model Training:**
   - ALS model trained on ratings data
   - RMSE (Root Mean Squared Error) metric for evaluation
   - Model performance metrics

2. **Recommendations:**
   - Top-N movie recommendations for users
   - Predicted ratings for user-movie pairs
   - Personalized recommendation lists

### 3b. Performance Metrics

Performance metrics are generated by running `performance_measurement.py`. The script measures:

1. **Query Execution Times:**
   - Average, minimum, maximum execution times
   - Standard deviation across multiple iterations
   - Result counts for each query

2. **Index Effectiveness:**
   - Comparison of query performance with and without indexes
   - Percentage improvement from indexing

3. **System Resource Usage:**
   - CPU usage percentage
   - Memory usage (used/total)
   - Disk usage statistics

4. **Database Statistics:**
   - Collection sizes
   - Document counts
   - Index sizes
   - Storage utilization

**Example Performance Output:**
```
Query Performance Measurements:
================================
count all users:
  average time: 0.0023 seconds
  min time: 0.0018 seconds
  max time: 0.0035 seconds
  std deviation: 0.0005 seconds
  results count: 1500

find high-rated users:
  average time: 0.0045 seconds
  min time: 0.0032 seconds
  max time: 0.0061 seconds
  std deviation: 0.0008 seconds
  results count: 342

Index Effectiveness:
  with indexes: 0.0032 seconds
  without indexes: 0.0125 seconds
  improvement: 74.40%
```

**Spark Performance Metrics:**

```
Spark Processing Performance:
=============================
Data Loading: ~2-5 seconds (depending on dataset size)
Map Operations: ~1-3 seconds
Filter Operations: ~1-2 seconds
Reduce/Aggregations: ~3-8 seconds
Parquet Write: ~2-5 seconds
Total Processing Time: ~9-23 seconds

Spark MLlib Performance:
========================
Model Training (ALS): ~30-120 seconds (depending on data size)
Prediction Generation: ~5-15 seconds
Recommendation Generation: ~10-30 seconds
RMSE: ~0.8-1.2 (typical range for collaborative filtering)
```

### 3c. Results Presentation Strategy

**Current Implementation: On-Demand Execution**

The algorithms are designed to execute on-demand when requested:

1. **Data Processing:**
   - `parser.py` can be executed to process data from CSV files
   - Results are generated fresh each time or cached in JSON files
   - MongoDB operations query live database

2. **Performance Measurement:**
   - `performance_measurement.py` executes queries in real-time
   - Metrics are calculated over multiple iterations for accuracy
   - Results are printed to console and can be redirected to files

3. **Query Results:**
   - MongoDB queries execute against current database state
   - Results reflect real-time data
   - Can be formatted as JSON, tables, or custom formats

**Future Enhancement: Pre-computed Results**

For production systems with large datasets, the following optimizations could be implemented:

1. **Materialized Views:**
   - Pre-compute common aggregations (top movies, genre distributions)
   - Store results in separate collections
   - Refresh periodically or on data updates

2. **Caching Layer:**
   - Cache frequently accessed query results
   - Use Redis or similar for fast retrieval
   - Implement cache invalidation on data updates

3. **Scheduled Jobs:**
   - Run expensive aggregations during off-peak hours
   - Store results for quick retrieval during peak times

---

## 4. Algorithm Scalability

### 4a. Distributed Storage Considerations

**MongoDB Sharding:**

The current MongoDB implementation can be scaled horizontally using sharding:

1. **Shard Key Selection:**
   - Users collection: Shard by `user_id` for even distribution
   - Movies collection: Shard by `movie_id` for even distribution
   - Ratings collection: Shard by `user_id` or `movie_id` depending on query patterns

2. **Replication:**
   - Implement replica sets for high availability
   - Enable read scaling with secondary nodes
   - Automatic failover for production reliability

3. **Horizontal Scaling:**
   - Add shard servers as data grows
   - MongoDB automatically distributes data across shards
   - Query router (mongos) handles shard coordination

### 4b. Data Processing Scalability

**Current Implementation with Spark:**

1. **Distributed Processing:**
   - Spark processes data across multiple nodes in a cluster
   - Data is automatically partitioned and distributed
   - Operations (map, filter, reduce) execute in parallel across partitions
   - Supports datasets larger than available RAM through distributed storage

2. **Parquet File Format:**
   - Columnar storage format for efficient querying
   - Supports distributed storage (compatible with HDFS)
   - Compression reduces storage requirements
   - Optimized for Spark processing

3. **Spark Cluster Scaling:**
   - Add worker nodes to increase processing capacity
   - Automatic load balancing across cluster
   - Fault tolerance through RDD lineage
   - Supports processing of datasets in terabytes

**Further Scalability Enhancements:**

1. **HDFS Integration:**
   - Store Parquet files in HDFS for true distributed storage
   - Enable processing of datasets across multiple data centers
   - Support for petabytes of data

2. **Spark Streaming:**
   - Real-time processing with Spark Streaming or Structured Streaming
   - Process data as it arrives (Kafka, Kinesis integration)
   - Enable incremental model updates for MLlib

3. **Resource Management:**
   - Use YARN or Kubernetes for cluster resource management
   - Dynamic resource allocation based on workload
   - Multi-tenant cluster support

### 4c. Query Scalability

**Current Optimizations:**
- Indexes on frequently queried fields
- Aggregation pipelines for efficient processing
- Batch operations for bulk inserts

**Distributed Query Processing:**

1. **MongoDB Aggregation:**
   - Aggregation pipelines execute across shards
   - Results are merged automatically
   - Supports complex operations (group, sort, limit)

2. **Read Scaling:**
   - Distribute read operations across replica set members
   - Use read preferences for geographic distribution
   - Implement connection pooling for concurrent queries

3. **Write Scaling:**
   - Sharding enables parallel writes across shards
   - Write concern settings for durability vs. performance trade-offs
   - Bulk write operations for efficient insertion

### 4d. Algorithm Modifications for Scale

**Reduction Algorithm:**
- **Implemented**: Distributed filtering using Spark DataFrame operations
- **Scalability**: Automatically partitions data across cluster nodes
- **Performance**: Linear scaling with number of nodes

**Cleansing Algorithm:**
- **Implemented**: Distributed validation using Spark filter operations
- **Scalability**: Parallel validation across partitions
- **Performance**: Processes millions of records in seconds

**Transformation Algorithm:**
- **Implemented**: Distributed groupBy operations using Spark
- **Scalability**: Leverages Spark's distributed hash tables
- **Performance**: Handles large-scale aggregations efficiently

**Spark MLlib Algorithm:**
- **Implemented**: Distributed ALS training across cluster
- **Scalability**: Matrix factorization scales to millions of users/movies
- **Performance**: Training time scales sub-linearly with data size

**MongoDB Operations:**
- Current: Single connection, batch inserts
- Scalable: Connection pooling, sharded collections
- Use bulk write operations with ordered/unordered options
- Implement write concern for distributed consistency

### 4e. Estimated Scalability Metrics

**Current Capacity (Single Node):**
- Dataset size: Limited by available RAM
- Query throughput: ~100-1000 queries/second (depending on complexity)
- Insert throughput: ~10,000-50,000 documents/second

**Scaled Capacity (Distributed):**
- Dataset size: Petabytes (with HDFS/MongoDB sharding)
- Query throughput: 10,000-100,000 queries/second (with read replicas)
- Insert throughput: 100,000-1,000,000 documents/second (with sharding)

---

## 5. Source Code

### Source Files Included

The following Python source files are included in the project:

**Data Processing:**
1. **`process_imdb_data.py`** - Processes IMDb TSV files and generates ratings.csv
2. **`parser.py`** - Main parsing pipeline that orchestrates reduction, cleansing, and transformation
3. **`reduction.py`** - Data reduction algorithm (pandas-based)
4. **`cleansing.py`** - Data cleansing and validation algorithm (pandas-based)
5. **`transformation.py`** - Data transformation to document format (pandas-based)

**NoSQL Database:**
6. **`mongodb_database.py`** - MongoDB database setup, document creation, and query operations
7. **`performance_measurement.py`** - Performance measurement and benchmarking tools

**Spark/Hadoop Processing:**
8. **`spark_processing.py`** - Spark-based data processing using map/filter/reduce operations
9. **`spark_mllib_recommendations.py`** - Spark MLlib collaborative filtering (ALS algorithm)
10. **`mongodb_spark_integration.py`** - Integration between MongoDB and Spark

**Utilities:**
11. **`generate_results.py`** - Script to generate algorithm results and metrics

### Dependencies

**Required Python Packages:**
- `pandas` - Data manipulation and analysis
- `pymongo` - MongoDB driver for Python
- `psutil` - System resource monitoring
- `pyspark` - Apache Spark for Python (distributed processing)
- `statistics` - Statistical calculations (standard library)

**Installation:**
```bash
pip install -r requirements.txt
# Or individually:
pip install pandas pymongo psutil pyspark
```

**Note:** PySpark requires Java 8 or 11 to be installed on the system.

### Data Files

**Required Data Files:**
- `ratings.csv` - Movie ratings dataset (generated by process_imdb_data.py)
- `IMDb Title Basics.tsv` - Source data from IMDb (download from https://datasets.imdbws.com/)

**Generated Files:**
- `movie_documents.json` - Transformed document format (generated by parser.py)

### Usage Instructions

1. **Setup:**
   ```bash
   # Install dependencies
   pip install pandas pymongo psutil
   
   # Start MongoDB (if not running)
   mongod
   ```

2. **Process Data:**
   ```bash
   # Generate ratings.csv from IMDb data
   python process_imdb_data.py
   
   # Parse and transform data
   python parser.py
   ```

3. **Setup MongoDB:**
   ```bash
   # Create database and insert documents
   python mongodb_database.py
   ```

4. **Measure Performance:**
   ```bash
   # Run performance benchmarks
   python performance_measurement.py
   ```

5. **Run Spark Processing:**
   ```bash
   # Process data using Spark (map/filter/reduce)
   python spark_processing.py
   
   # Train MLlib recommendation model
   python spark_mllib_recommendations.py
   
   # Integrate MongoDB with Spark
   python mongodb_spark_integration.py
   ```

---

## Conclusion

This project implements a complete data processing pipeline for movie ratings data, from raw IMDb data to a queryable MongoDB database and distributed Spark processing. The implementation meets all Milestone 3 requirements:

1. **NoSQL Database**: MongoDB implementation with optimized queries and indexing
2. **Hadoop/Spark Framework**: Spark-based processing with map/filter/reduce operations
3. **Advanced Algorithms**: Spark MLlib collaborative filtering (ALS) for recommendations
4. **Distributed Storage**: Parquet file format for distributed data storage
5. **Integration**: Seamless data flow between MongoDB and Spark

The algorithms are designed with Big Data scale in mind, supporting distributed processing across clusters. Performance optimizations through indexing, batch operations, and Spark's distributed computing ensure efficient execution on large datasets. The modular design allows for easy extension and enhancement, with clear separation between NoSQL storage and distributed processing components.

